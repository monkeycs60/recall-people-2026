# Backend TODO

## Configuration AI Text Provider (G√©n√©ration de texte)

### Pour activer le provider Cerebras:

1. **Ajouter la cl√© API Cerebras dans votre fichier .env:**
   ```
   CEREBRAS_API_KEY=votre-cl√©-cerebras-ici
   ```

2. **Changer le provider AI dans .env:**
   ```
   AI_PROVIDER=cerebras
   ```

3. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Switching entre text providers:

- **Pour utiliser Grok (d√©faut):**
  ```
  AI_PROVIDER=grok
  ```

- **Pour utiliser Cerebras:**
  ```
  AI_PROVIDER=cerebras
  ```

### Models utilis√©s (Text):

- **Grok:** `grok-4-1-fast`
- **Cerebras:** `gpt-oss-120b`

---

## Configuration Speech-to-Text Provider (Transcription)

### Pour activer Groq Whisper v3 Turbo (RECOMMAND√â):

1. **Ajouter la cl√© API Groq dans votre fichier .env:**
   ```
   GROQ_API_KEY=votre-cl√©-groq-ici
   ```

2. **Changer le provider STT dans .env:**
   ```
   STT_PROVIDER=groq-whisper-v3-turbo
   ```

3. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Switching entre STT providers:

- **Pour utiliser Deepgram Nova-3 (d√©faut):**
  ```
  STT_PROVIDER=deepgram
  ```

- **Pour utiliser Groq Whisper Large v3:**
  ```
  STT_PROVIDER=groq-whisper-v3
  ```

- **Pour utiliser Groq Whisper v3 Turbo (recommand√© - 8x plus rapide):**
  ```
  STT_PROVIDER=groq-whisper-v3-turbo
  ```

### Models utilis√©s (Speech-to-Text):

- **Deepgram:** `nova-3` (rapide, fiable)
- **Groq Whisper v3:** `whisper-large-v3` (pr√©cision maximale)
- **Groq Whisper v3 Turbo:** `whisper-large-v3-turbo` (meilleur √©quilibre vitesse/pr√©cision)

### Comparaison des providers STT:

| Provider | Vitesse | Pr√©cision | Co√ªt | Recommand√© pour |
|----------|---------|-----------|------|-----------------|
| Deepgram Nova-3 | ‚ö°‚ö°‚ö° Tr√®s rapide | ‚≠ê‚≠ê‚≠ê Excellente | üí∞üí∞ Moyen | Production stable |
| Groq Whisper v3 | ‚ö°‚ö° Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê Maximale | üí∞ √âconomique | Pr√©cision maximale |
| Groq Whisper v3 Turbo | ‚ö°‚ö°‚ö°‚ö° Ultra rapide | ‚≠ê‚≠ê‚≠ê Excellente | üí∞ √âconomique | **Usage g√©n√©ral** ‚úÖ |

### Notes:

- Le syst√®me switche automatiquement entre providers selon les variables d'environnement
- Si `AI_PROVIDER` n'est pas d√©fini, le syst√®me utilise Grok par d√©faut
- Si `STT_PROVIDER` n'est pas d√©fini, le syst√®me utilise Deepgram par d√©faut
- Les providers text utilisent le Vercel AI SDK
- Groq Whisper est g√©n√©ralement plus rapide et moins cher que Deepgram
- Tous les appels AI text sont centralis√©s dans `/backend/src/lib/ai-provider.ts`
- Tous les appels STT sont centralis√©s dans `/backend/src/lib/speech-to-text-provider.ts`

---

## Performance Logging System

### Activer le logging de performance:

1. **Dans votre fichier .env:**
   ```
   ENABLE_PERFORMANCE_LOGGING=true
   ```

2. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Ce qui est logg√©:

Le syst√®me de logging mesure et affiche pour chaque appel AI/STT :

- ‚úÖ **Provider** : quel provider est utilis√© (grok, cerebras, deepgram, groq-whisper-v3, etc.)
- ‚úÖ **Model** : quel mod√®le sp√©cifique (grok-4-1-fast, gpt-oss-120b, nova-3, whisper-large-v3-turbo)
- ‚úÖ **Route** : quelle route API (/transcribe, /extract, /search, etc.)
- ‚úÖ **Duration** : temps total de l'op√©ration en millisecondes
- ‚úÖ **Input size** : taille de l'input en bytes
- ‚úÖ **Output size** : taille de l'output en bytes
- ‚úÖ **Success** : si l'op√©ration a r√©ussi ou √©chou√©
- ‚úÖ **Metadata** : informations contextuelles (langue, nombre de facts, etc.)

### Format des logs:

```
‚úÖ PERFORMANCE LOG [10:30:45]
Route: /transcribe
Provider: groq-whisper-v3-turbo (whisper-large-v3-turbo)
Operation: speech-to-text
‚è±Ô∏è  Duration: 450ms
Input: 245.5 KB
Output: 1.2 KB
Metadata: { language: 'fr' }
```

### Cas d'usage - Flow principal (Audio ‚Üí Transcription ‚Üí Extraction):

Quand tu enregistres un audio et s√©lectionnes un contact :

1. **`/transcribe`** : Log du temps de transcription (STT provider)
   - Compare Deepgram vs Groq Whisper v3 Turbo

2. **`/extract`** : Log du temps d'extraction (AI provider)
   - Compare Grok vs Cerebras

**Temps total du flow** = Temps transcription + Temps extraction

### Comparer les providers:

Pour comparer les performances entre providers :

1. **Activer le logging** (`ENABLE_PERFORMANCE_LOGGING=true`)

2. **Tester avec provider 1:**
   ```
   STT_PROVIDER=deepgram
   AI_PROVIDER=grok
   ```
   ‚Üí Enregistrer un audio et noter les temps

3. **Tester avec provider 2:**
   ```
   STT_PROVIDER=groq-whisper-v3-turbo
   AI_PROVIDER=cerebras
   ```
   ‚Üí Enregistrer le m√™me type d'audio et comparer

4. **Analyser les r√©sultats** dans la console pour voir quel combo est le plus rapide

### Routes logg√©es:

- `/transcribe` - Speech-to-text (operation: speech-to-text)
- `/extract` - Extraction d'infos (operation: object-generation)
- `/search` - Recherche s√©mantique (operation: object-generation)
- `/summary` - R√©sum√© de contact (operation: text-generation)
- `/similarity` - Calcul de similarit√© (operation: object-generation)

### D√©sactiver le logging:

```
ENABLE_PERFORMANCE_LOGGING=false
```

Ou simplement enlever la variable de .env

---

## LangFuse LLM Observability & Evaluations

### Vue d'ensemble

LangFuse fournit une observabilit√© compl√®te pour les appels LLM via OpenTelemetry :

- ‚úÖ **Traces compl√®tes** : Input/output de chaque appel LLM
- ‚úÖ **Co√ªts automatiques** : Calcul des co√ªts par provider/mod√®le
- ‚úÖ **Latency metrics** : p50, p95, p99
- ‚úÖ **Quality scores** : √âvaluations LLM-as-a-judge
- ‚úÖ **Comparaison providers** : Grok vs Cerebras en production
- ‚úÖ **Dashboard visuel** : Toutes les m√©triques en temps r√©el

### Setup initial

#### 1. Cr√©er un compte LangFuse (gratuit)

1. Aller sur **https://cloud.langfuse.com**
2. Cr√©er un compte gratuit
3. Cr√©er un nouveau projet
4. Aller dans Settings ‚Üí API Keys

#### 2. R√©cup√©rer les cl√©s API

Copier les cl√©s suivantes :
- `LANGFUSE_SECRET_KEY` (commence par `sk-lf-...`)
- `LANGFUSE_PUBLIC_KEY` (commence par `pk-lf-...`)

#### 3. Configurer les variables d'environnement

**Dans votre fichier .env local :**
```bash
ENABLE_LANGFUSE=true
LANGFUSE_SECRET_KEY=sk-lf-votre-secret-key
LANGFUSE_PUBLIC_KEY=pk-lf-votre-public-key
LANGFUSE_BASE_URL=https://cloud.langfuse.com
```

**Pour Cloudflare Workers (production) :**

Via le dashboard Cloudflare :
1. Aller dans Workers & Pages ‚Üí Votre worker ‚Üí Settings ‚Üí Variables
2. Ajouter les 4 variables ci-dessus

Ou via CLI :
```bash
wrangler secret put LANGFUSE_SECRET_KEY
wrangler secret put LANGFUSE_PUBLIC_KEY
# Puis ajouter les autres dans wrangler.toml
```

#### 4. Tester en dev

```bash
npm run dev
# Faire quelques appels AI (transcription, extraction, etc.)
# V√©rifier le dashboard LangFuse : https://cloud.langfuse.com
```

### Architecture technique

**OpenTelemetry + LangFuse :**

```
Vercel AI SDK (generateText/Object)
    ‚Üì G√©n√®re spans OpenTelemetry
NodeTracerProvider
    ‚Üì Capture les spans
LangfuseSpanProcessor
    ‚Üì Transforme et envoie
LangFuse Cloud API
    ‚Üì
Dashboard LangFuse
```

**Fichiers cl√©s :**
- `backend/src/lib/telemetry.ts` - Setup OpenTelemetry
- `backend/src/middleware/langfuse.ts` - Middleware avec flush
- `backend/src/lib/evaluators.ts` - LLM-as-a-judge evaluators

### LLM-as-a-Judge Evaluators

Le syst√®me inclut 3 evaluators automatiques pour mesurer la qualit√© :

#### 1. evaluateExtraction()

**Mesure :** Qualit√© de l'extraction vs transcription

**Crit√®res :**
- ‚úÖ Compl√©tude : Tous les contacts mentionn√©s sont extraits ?
- ‚úÖ Exactitude : Les infos extraites sont correctes ?
- ‚úÖ Pas d'hallucinations : Rien d'invent√© ?
- ‚úÖ Contexte pr√©serv√© : Relations/notes bien captur√©es ?

**Usage :**
```typescript
import { evaluateExtraction } from '../lib/evaluators';

const evaluation = await evaluateExtraction(
  transcription,
  extraction,
  {
    XAI_API_KEY: c.env.XAI_API_KEY,
    enableEvaluation: c.env.ENABLE_LANGFUSE === 'true',
    samplingRate: 0.25 // 25% = 1 sur 4
  }
);

if (evaluation) {
  console.log(`Extraction quality: ${evaluation.score}/10`);
  console.log(`Reasoning: ${evaluation.reasoning}`);
}
```

#### 2. evaluateSummary()

**Mesure :** Pertinence et utilit√© des r√©sum√©s de contacts

**Crit√®res :**
- ‚úÖ Pertinence : R√©sum√© capture les points cl√©s ?
- ‚úÖ Concision : Ni trop long, ni trop court ?
- ‚úÖ Exactitude : Pas d'infos fausses ?
- ‚úÖ Utilit√© : Aide √† comprendre le contact ?

**Usage :**
```typescript
import { evaluateSummary } from '../lib/evaluators';

const evaluation = await evaluateSummary(
  { facts, hotTopics },
  summary,
  {
    XAI_API_KEY: c.env.XAI_API_KEY,
    enableEvaluation: c.env.ENABLE_LANGFUSE === 'true',
    samplingRate: 0.25
  }
);
```

#### 3. evaluateSearch()

**Mesure :** Qualit√© des r√©sultats de recherche

**Crit√®res :**
- ‚úÖ Pertinence : R√©sultats correspondent √† la query ?
- ‚úÖ Exactitude : R√©ponses viennent des sources (facts/memories/notes) ?
- ‚úÖ Scores coh√©rents : Les relevanceScore sont bien calibr√©s ?
- ‚úÖ Pas d'hallucinations : Rien d'invent√© ?

**Usage :**
```typescript
import { evaluateSearch } from '../lib/evaluators';

const evaluation = await evaluateSearch(
  query,
  { facts, memories, notes },
  searchResults,
  {
    XAI_API_KEY: c.env.XAI_API_KEY,
    enableEvaluation: c.env.ENABLE_LANGFUSE === 'true',
    samplingRate: 0.25
  }
);
```

### Configuration du sampling

**Par d√©faut : 25% (1 sur 4)**

Le sampling r√©duit les co√ªts d'√©valuation de 75% tout en gardant une bonne visibilit√©.

**Modifier le taux de sampling :**

```typescript
// 10% = 1 sur 10 (encore moins cher)
samplingRate: 0.1

// 50% = 1 sur 2 (plus de donn√©es)
samplingRate: 0.5

// 100% = tout √©valuer (pour debug)
samplingRate: 1.0
```

**Configurer dans le code :**
- `backend/src/lib/evaluators.ts:36` - Type par d√©faut
- `backend/src/lib/evaluators.ts:45` - Fonction shouldEvaluate()

### Co√ªts

| Item | Co√ªt |
|------|------|
| **LangFuse Cloud (free tier)** | $0 - 50k observations/mois |
| **LangFuse Pro** | $59/mois - Plus de volume |
| **LLM-as-a-judge (Grok 4.1 Fast)** | ~$0.15 / 1000 √©valuations |
| **Avec sampling 25%** | R√©duit co√ªts eval de 75% |

**Exemple :** Si tu fais 10,000 extractions/mois avec sampling 25% :
- Observations LangFuse : 10,000 (gratuit)
- √âvaluations : 2,500 (10k √ó 25%) = ~$0.38/mois

### Dashboard LangFuse

**URL :** https://cloud.langfuse.com

**Onglets disponibles :**

1. **Traces** : Tous les appels LLM avec input/output complets
2. **Sessions** : Regroupement par utilisateur/conversation
3. **Metrics** : Graphiques de latency, co√ªts, volumes
4. **Scores** : R√©sultats des √©valuations qualit√©
5. **Users** : Analytics par utilisateur
6. **Prompts** : Gestion et versioning des prompts

**Filtres utiles :**
- Par route : `/extract`, `/search`, `/summary`
- Par provider : `grok`, `cerebras`
- Par score qualit√© : `< 7/10` (pour voir les probl√®mes)
- Par dur√©e : `> 2s` (pour optimiser les lents)

### Comparer Grok vs Cerebras en prod

Avec LangFuse activ√©, tu peux facilement comparer :

1. **Dashboard ‚Üí Metrics ‚Üí Filter by provider**
2. Voir pour chaque provider :
   - Latency moyenne (p50, p95, p99)
   - Co√ªts totaux
   - Quality scores moyens
   - Volume d'utilisation

3. **Tester les deux :**
   ```bash
   # Semaine 1 : Grok
   AI_PROVIDER=grok

   # Semaine 2 : Cerebras
   AI_PROVIDER=cerebras
   ```

4. **Analyser dans le dashboard** :
   - Quel provider est le plus rapide ?
   - Quel provider a les meilleurs quality scores ?
   - Quel provider co√ªte le moins cher ?

### D√©sactiver LangFuse

```bash
ENABLE_LANGFUSE=false
```

Ou simplement enlever la variable de .env

**Note :** Le syst√®me continue de fonctionner normalement sans LangFuse. L'observabilit√© est optionnelle.

### Self-hosting (optionnel)

Si tu veux h√©berger LangFuse toi-m√™me (gratuit, open source MIT) :

1. Suivre la doc : https://langfuse.com/docs/deployment/self-host
2. Deploy via Docker, Railway, ou Render
3. Changer `LANGFUSE_BASE_URL` vers ton instance

### Prochaines √©tapes recommand√©es

- [ ] Cr√©er compte LangFuse gratuit
- [ ] Ajouter les cl√©s API dans `.env`
- [ ] Activer `ENABLE_LANGFUSE=true`
- [ ] Tester quelques appels en dev
- [ ] Explorer le dashboard
- [ ] Optionnel : Int√©grer evaluators dans les routes
- [ ] Optionnel : Cr√©er datasets de test
- [ ] D√©ployer en production avec les secrets Cloudflare
