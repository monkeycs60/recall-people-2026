# Backend TODO

## Configuration AI Text Provider (G√©n√©ration de texte)

### Pour activer le provider Cerebras:

1. **Ajouter la cl√© API Cerebras dans votre fichier .env:**
   ```
   CEREBRAS_API_KEY=votre-cl√©-cerebras-ici
   ```

2. **Changer le provider AI dans .env:**
   ```
   AI_PROVIDER=cerebras
   ```

3. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Switching entre text providers:

- **Pour utiliser Grok (d√©faut):**
  ```
  AI_PROVIDER=grok
  ```

- **Pour utiliser Cerebras:**
  ```
  AI_PROVIDER=cerebras
  ```

### Models utilis√©s (Text):

- **Grok:** `grok-4-1-fast`
- **Cerebras:** `gpt-oss-120b`

---

## Configuration Speech-to-Text Provider (Transcription)

### Pour activer Groq Whisper v3 Turbo (RECOMMAND√â):

1. **Ajouter la cl√© API Groq dans votre fichier .env:**
   ```
   GROQ_API_KEY=votre-cl√©-groq-ici
   ```

2. **Changer le provider STT dans .env:**
   ```
   STT_PROVIDER=groq-whisper-v3-turbo
   ```

3. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Switching entre STT providers:

- **Pour utiliser Deepgram Nova-3 (d√©faut):**
  ```
  STT_PROVIDER=deepgram
  ```

- **Pour utiliser Groq Whisper Large v3:**
  ```
  STT_PROVIDER=groq-whisper-v3
  ```

- **Pour utiliser Groq Whisper v3 Turbo (recommand√© - 8x plus rapide):**
  ```
  STT_PROVIDER=groq-whisper-v3-turbo
  ```

### Models utilis√©s (Speech-to-Text):

- **Deepgram:** `nova-3` (rapide, fiable)
- **Groq Whisper v3:** `whisper-large-v3` (pr√©cision maximale)
- **Groq Whisper v3 Turbo:** `whisper-large-v3-turbo` (meilleur √©quilibre vitesse/pr√©cision)

### Comparaison des providers STT:

| Provider | Vitesse | Pr√©cision | Co√ªt | Recommand√© pour |
|----------|---------|-----------|------|-----------------|
| Deepgram Nova-3 | ‚ö°‚ö°‚ö° Tr√®s rapide | ‚≠ê‚≠ê‚≠ê Excellente | üí∞üí∞ Moyen | Production stable |
| Groq Whisper v3 | ‚ö°‚ö° Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê Maximale | üí∞ √âconomique | Pr√©cision maximale |
| Groq Whisper v3 Turbo | ‚ö°‚ö°‚ö°‚ö° Ultra rapide | ‚≠ê‚≠ê‚≠ê Excellente | üí∞ √âconomique | **Usage g√©n√©ral** ‚úÖ |

### Notes:

- Le syst√®me switche automatiquement entre providers selon les variables d'environnement
- Si `AI_PROVIDER` n'est pas d√©fini, le syst√®me utilise Grok par d√©faut
- Si `STT_PROVIDER` n'est pas d√©fini, le syst√®me utilise Deepgram par d√©faut
- Les providers text utilisent le Vercel AI SDK
- Groq Whisper est g√©n√©ralement plus rapide et moins cher que Deepgram
- Tous les appels AI text sont centralis√©s dans `/backend/src/lib/ai-provider.ts`
- Tous les appels STT sont centralis√©s dans `/backend/src/lib/speech-to-text-provider.ts`

---

## Performance Logging System

### Activer le logging de performance:

1. **Dans votre fichier .env:**
   ```
   ENABLE_PERFORMANCE_LOGGING=true
   ```

2. **Red√©marrer le serveur:**
   ```
   npm run dev
   ```

### Ce qui est logg√©:

Le syst√®me de logging mesure et affiche pour chaque appel AI/STT :

- ‚úÖ **Provider** : quel provider est utilis√© (grok, cerebras, deepgram, groq-whisper-v3, etc.)
- ‚úÖ **Model** : quel mod√®le sp√©cifique (grok-4-1-fast, gpt-oss-120b, nova-3, whisper-large-v3-turbo)
- ‚úÖ **Route** : quelle route API (/transcribe, /extract, /search, etc.)
- ‚úÖ **Duration** : temps total de l'op√©ration en millisecondes
- ‚úÖ **Input size** : taille de l'input en bytes
- ‚úÖ **Output size** : taille de l'output en bytes
- ‚úÖ **Success** : si l'op√©ration a r√©ussi ou √©chou√©
- ‚úÖ **Metadata** : informations contextuelles (langue, nombre de facts, etc.)

### Format des logs:

```
‚úÖ PERFORMANCE LOG [10:30:45]
Route: /transcribe
Provider: groq-whisper-v3-turbo (whisper-large-v3-turbo)
Operation: speech-to-text
‚è±Ô∏è  Duration: 450ms
Input: 245.5 KB
Output: 1.2 KB
Metadata: { language: 'fr' }
```

### Cas d'usage - Flow principal (Audio ‚Üí Transcription ‚Üí Extraction):

Quand tu enregistres un audio et s√©lectionnes un contact :

1. **`/transcribe`** : Log du temps de transcription (STT provider)
   - Compare Deepgram vs Groq Whisper v3 Turbo

2. **`/extract`** : Log du temps d'extraction (AI provider)
   - Compare Grok vs Cerebras

**Temps total du flow** = Temps transcription + Temps extraction

### Comparer les providers:

Pour comparer les performances entre providers :

1. **Activer le logging** (`ENABLE_PERFORMANCE_LOGGING=true`)

2. **Tester avec provider 1:**
   ```
   STT_PROVIDER=deepgram
   AI_PROVIDER=grok
   ```
   ‚Üí Enregistrer un audio et noter les temps

3. **Tester avec provider 2:**
   ```
   STT_PROVIDER=groq-whisper-v3-turbo
   AI_PROVIDER=cerebras
   ```
   ‚Üí Enregistrer le m√™me type d'audio et comparer

4. **Analyser les r√©sultats** dans la console pour voir quel combo est le plus rapide

### Routes logg√©es:

- `/transcribe` - Speech-to-text (operation: speech-to-text)
- `/extract` - Extraction d'infos (operation: object-generation)
- `/search` - Recherche s√©mantique (operation: object-generation)
- `/summary` - R√©sum√© de contact (operation: text-generation)
- `/similarity` - Calcul de similarit√© (operation: object-generation)

### D√©sactiver le logging:

```
ENABLE_PERFORMANCE_LOGGING=false
```

Ou simplement enlever la variable de .env
